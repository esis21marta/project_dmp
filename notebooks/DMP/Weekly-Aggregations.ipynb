{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as t\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "conf = (SparkConf()\n",
    "        .setMaster('yarn-client')\n",
    "        .setAppName('all-domain-weekly-agg')\n",
    "        .set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "        .set(\"spark.driver.memory\", \"16g\")\n",
    "        .set(\"spark.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "        .set(\"spark.dynamicAllocation.maxExecutors\", \"25\")\n",
    "        .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "        .set(\"spark.executor.cores\", \"4\")\n",
    "        .set(\"spark.executor.memory\", \"16g\")\n",
    "        .set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "        .set(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "        .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .set(\"spark.sql.broadcastTimeout\", \"1000\")\n",
    "        .set(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n",
    "        .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "        .set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "        .set(\"spark.yarn.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.yarn.maxAppAttempts\", \"2\")\n",
    "        .set(\"spark.yarn.queue\", \"root.hue_dmp\")\n",
    "        .set(\"yarn.nodemanager.vmem-check-enabled\", \"false\")\n",
    "        )\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import filter_latest_data, next_week_start_day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_size_0 = 1\n",
    "partition_size_1 = 100\n",
    "partition_size_2 = 200\n",
    "partition_size_3 = 300\n",
    "partition_size_4 = 500\n",
    "partition_size_5 = 1000\n",
    "partition_size_6 = 2000\n",
    "partition_size_7 = 5000\n",
    "partition_size_8 = 10000\n",
    "partition_size_9 = 20000\n",
    "partition_size_10 = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airtime Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_format = 'yyyy-MM-dd HH:mm:ss'\n",
    "\n",
    "\n",
    "def fea_calc_airtime_loan_count_features(takers_df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    takers_df_weekstart = (takers_df\n",
    "                           .withColumn(\"trx_date\", from_unixtime(unix_timestamp(f.col('timestamp'), timestamp_format)))\n",
    "                           .withColumn(\"weekstart\", next_week_start_day(\"trx_date\"))\n",
    "                           )\n",
    "\n",
    "    takers_with_count_df = (takers_df_weekstart\n",
    "                            .groupBy(\"msisdn\", \"weekstart\")\n",
    "                            .agg(f.count(f.when(f.col(\"campaignid\").isNotNull(), True)).alias(\"tot_airtime_loan_count\"))\n",
    "                            )\n",
    "\n",
    "    return takers_df_weekstart.join(takers_with_count_df, [\"msisdn\", \"weekstart\"], \"left\")\n",
    "\n",
    "\n",
    "def compute_dpd_per_weeekstart(takers_df: pyspark.sql.DataFrame,\n",
    "                               payment_df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "\n",
    "    # Left join takers and payment with msisdn, campaignid, month campaign\n",
    "    takers_payment_joined = takers_df.alias('takers_df').join(\n",
    "        payment_df.alias('payment_df'),\n",
    "        (f.col('takers_df.msisdn') == f.col('payment_df.msisdn')) & (\n",
    "                    f.col('takers_df.campaignid').substr(22, 4) == f.col('payment_df.campaignid').substr(24, 4)) & (\n",
    "                    f.col('takers_df.month_campaign') == f.col('payment_df.month_campaign')),\n",
    "        'left_outer'\n",
    "    )\n",
    "\n",
    "    # Convert timestamp columns for takers and payment\n",
    "    takers_payment_joined_cleaned = takers_payment_joined.select(\n",
    "        'takers_df.msisdn',\n",
    "        'takers_df.weekstart',\n",
    "        'takers_df.trx_date',\n",
    "        f.col('takers_df.tot_airtime_loan_count'),\n",
    "        from_unixtime(unix_timestamp(f.col('takers_df.timestamp'), timestamp_format)).alias('taker_timestamp'),\n",
    "        from_unixtime(unix_timestamp(f.col('payment_df.timestamp'), timestamp_format)).alias('repayment_timestamp')\n",
    "    )\n",
    "\n",
    "    # Compute duration between dates (excluding start, including end)\n",
    "    paid_takers_df = (takers_payment_joined_cleaned\n",
    "                      # .filter(f.col('repayment_timestamp').isNotNull())\n",
    "                      .withColumn('repayment_duration',\n",
    "                                  f.datediff(f.col('repayment_timestamp'), f.col('taker_timestamp')))\n",
    "                      )\n",
    "\n",
    "    paid_takers_df_weekly = (paid_takers_df.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "            f.first(f.col(\"tot_airtime_loan_count\")).alias(\"tot_airtime_loan_count\"),\n",
    "            f.max(f.col(\"repayment_duration\")).alias(\"repayment_duration\"),\n",
    "\n",
    "            f.when(f.max(f.col(\"repayment_duration\")).isNull(), None)\n",
    "                .otherwise(f.count(f.when(f.col(\"repayment_duration\") > 2, True))).alias(\"2_dpd_count\"),\n",
    "            f.when(f.max(f.col(\"repayment_duration\")).isNull(), None)\n",
    "                .otherwise(f.count(f.when(f.col(\"repayment_duration\") > 10, True))).alias(\"10_dpd_count\"),\n",
    "            f.when(f.max(f.col(\"repayment_duration\")).isNull(), None)\n",
    "                .otherwise(f.count(f.when(f.col(\"repayment_duration\") > 15, True))).alias(\"15_dpd_count\"),\n",
    "            f.when(f.max(f.col(\"repayment_duration\")).isNull(), None)\n",
    "                .otherwise(f.count(f.when(f.col(\"repayment_duration\") > 20, True))).alias(\"20_dpd_count\"),\n",
    "            f.when(f.max(f.col(\"repayment_duration\")).isNull(), None)\n",
    "                .otherwise(f.count(f.when(f.col(\"repayment_duration\") > 25, True))).alias(\"25_dpd_count\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return paid_takers_df_weekly\n",
    "\n",
    "\n",
    "def create_weekly_airtime_loan_table(\n",
    "    df_airtime_loan_takers: pyspark.sql.DataFrame,\n",
    "    df_airtime_loan_payment: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "\n",
    "    takers_df_with_atl_count = fea_calc_airtime_loan_count_features(df_airtime_loan_takers)\n",
    "    airtime_loan_weekly = compute_dpd_per_weeekstart(takers_df_with_atl_count, df_airtime_loan_payment)\n",
    "\n",
    "    return airtime_loan_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Internet Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bcp_usage_weekly(\n",
    "    df_smy_bcp_usage_dd: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates Weekly BCP Usage for the partner MSISDNs and required categories\n",
    "\n",
    "    :param df_smy_bcp_usage_dd: BCP Daily Usage DataFrame\n",
    "\n",
    "    :param is_incremental: Boolean Flag for Incremental Approach\n",
    "\n",
    "    :param partition_num: Number of Partition\n",
    "\n",
    "    :return: BCP Weekly Usage DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df_bcp = df_smy_bcp_usage_dd.withColumn(\n",
    "        \"trx_date\", f.to_date(f.col(\"trx_date\").cast(t.StringType()), \"yyyyMMdd\")\n",
    "    ).withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "\n",
    "    # Aggregating weekly\n",
    "    df_bcp_usage_weekly = df_bcp.groupBy(\n",
    "        \"msisdn\", \"weekstart\", \"accessed_app\", \"component\"\n",
    "    ).agg(\n",
    "        f.sum(f.coalesce(\"volume_in\", f.lit(0))).alias(\"volume_in\"),\n",
    "        f.sum(f.coalesce(\"volume_out\", f.lit(0))).alias(\"volume_out\"),\n",
    "        f.min(\"trx_date\").alias(\"min_trx_date\"),\n",
    "        f.max(\"trx_date\").alias(\"max_trx_date\"),\n",
    "    )\n",
    "\n",
    "    return df_bcp_usage_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bcp_weekly_feature_aggregation(\n",
    "    df_bcp_usage_weekly: pyspark.sql.DataFrame,\n",
    "    df_bcp_feature_mapping: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates Weekly BCP Usage for the partner MSISDNs and required categories\n",
    "\n",
    "    :param df_bcp_usage_weekly: BCP Weekly Usage DataFrame with group By Features\n",
    "\n",
    "    :param df_bcp_feature_mapping: BCP Mapping\n",
    "\n",
    "    :param is_incremental: Boolean Flag for Incremental Approach\n",
    "\n",
    "    :return: BCP Weekly Usage DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Get All Categories\n",
    "    df_categories = df_bcp_feature_mapping.select(\n",
    "        \"accessed_app\", \"component\", \"category\"\n",
    "    ).distinct()\n",
    "\n",
    "    df_bcp = df_bcp_usage_weekly.join(\n",
    "        f.broadcast(df_categories), [\"accessed_app\", \"component\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Aggregating weekly\n",
    "    df_bcp_usage_weekly = df_bcp.groupBy(\"msisdn\", \"weekstart\", \"category\").agg(\n",
    "        f.sum(f.coalesce(\"volume_in\", f.lit(0))).alias(\"volume_in\"),\n",
    "        f.sum(f.coalesce(\"volume_out\", f.lit(0))).alias(\"volume_out\"),\n",
    "        f.collect_set(f.col(\"accessed_app\")).alias(\"accessed_apps_01w\"),\n",
    "        f.min(\"min_trx_date\").alias(\"min_trx_date\"),\n",
    "        f.max(\"max_trx_date\").alias(\"max_trx_date\"),\n",
    "    )\n",
    "\n",
    "    return df_bcp_usage_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_customer_profile_to_weekly(\n",
    "    cb_multidim_df: pyspark.sql.DataFrame, partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns customer loyalty tier and points usage features\n",
    "    :param cb_multidim_df: Multidim table\n",
    "    :return: cb_multidim_df with msisdn, weekstart and selected features (in select statement below)\n",
    "    \"\"\"\n",
    "    multidim_weekstart = (\n",
    "        cb_multidim_df.withColumn(\n",
    "            \"trx_date\", f.to_date(f.col(\"trx_date\").cast(t.StringType()), \"yyyy-MM-dd\")\n",
    "        )\n",
    "        .withColumn(\"weekday\", f.date_format(f.col(\"trx_date\"), \"EEE\"))\n",
    "        .filter(f.col(\"weekday\") == \"Sun\")\n",
    "    )\n",
    "    customer_profile_output = multidim_weekstart.select(\n",
    "        f.col(\"msisdn\"),\n",
    "        next_week_start_day(f.col(\"trx_date\")).alias(\"weekstart\"),\n",
    "        f.col(\"lte_usim_user_flag\").alias(\"fea_custprof_lte_usim_user_flag\"),\n",
    "        f.col(\"area_sales\").alias(\"fea_custprof_area_sales\"),\n",
    "        f.col(\"region_sales\").alias(\"fea_custprof_region_sales\"),\n",
    "        f.col(\"bill_responsibility_type\").alias(\n",
    "            \"fea_custprof_bill_responsibility_type\"\n",
    "        ),\n",
    "        f.col(\"segment_data_user\").alias(\"fea_custprof_segment_data_user\"),\n",
    "        f.col(\"los\").alias(\"fea_custprof_los\"),\n",
    "        f.col(\"status\").alias(\"fea_custprof_status\"),\n",
    "        f.col(\"brand\").alias(\"fea_custprof_brand\"),\n",
    "        f.col(\"price_plan\").alias(\"fea_custprof_price_plan\"),\n",
    "        f.col(\"cust_type_desc\").alias(\"fea_custprof_cust_type_desc\"),\n",
    "        f.col(\"cust_subtype_desc\").alias(\"fea_custprof_cust_subtype_desc\"),\n",
    "        f.col(\"segment_hvc_mtd\").alias(\"fea_custprof_segment_hvc_mtd\"),\n",
    "        f.col(\"segment_hvc_m1\").alias(\"fea_custprof_segment_hvc_m1\"),\n",
    "        f.col(\"loyalty_tier\").alias(\"fea_custprof_loyalty_tier\"),\n",
    "        f.col(\"nik_gender\").alias(\"fea_custprof_nik_gender\"),\n",
    "        f.col(\"nik_age\").alias(\"fea_custprof_nik_age\"),\n",
    "        f.col(\"bill_cycle\").alias(\"fea_custprof_bill_cycle\"),\n",
    "        f.col(\"persona_los\").alias(\"fea_custprof_persona_los\"),\n",
    "        f.col(\"prsna_quadrant\").alias(\"fea_custprof_prsna_quadrant\"),\n",
    "        f.col(\"arpu_segment_name\").alias(\"fea_custprof_arpu_segment_name\"),\n",
    "        f.col(\"mytsel_user_flag\").alias(\"fe_custprofa_mytsel_user_flag\"),\n",
    "    )\n",
    "    return customer_profile_output.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_handset_lookup_data(\n",
    "    df_handset_dim: pyspark.sql.DataFrame, partition_num: int\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters any one device per IMEI\n",
    "    :param df_handset_dim: Lookup static table for each IMEI\n",
    "    :param partition_num: Number of Partition\n",
    "    :return df_device_lookup: One row (device) per IMEI\n",
    "    \"\"\"\n",
    "\n",
    "    # Selecting one device per IMEI\n",
    "    df_handset_dim = df_handset_dim.withColumn(\n",
    "        \"row_number\",\n",
    "        f.row_number().over(Window.partitionBy(\"tac\").orderBy(\"device_type\")),\n",
    "    )\n",
    "\n",
    "    df_handset_lookup = df_handset_dim.filter(f.col(\"row_number\") == 1).select(\n",
    "        \"tac\",\n",
    "        \"manufacturer\",\n",
    "        \"market_name\",\n",
    "        \"device_type\",\n",
    "        \"band\",\n",
    "        \"gprs_capable_flag\",\n",
    "        \"edge_capable_flag\",\n",
    "        \"umts_capable_flag\",\n",
    "        \"hsdpa_capable_flag\",\n",
    "        f.col(\"lte_capable_flag\").alias(\"dim_lte_capable_flag\"),\n",
    "        \"highest_network\",\n",
    "        \"data_capable_flag\",\n",
    "        \"wlan_capable_flag\",\n",
    "        \"bluetooth_capable_flag\",\n",
    "        \"dual_sim_flag\",\n",
    "        \"nfc_capable_flag\",\n",
    "        \"form_factor\",\n",
    "        \"os_version\",\n",
    "        \"os_name\",\n",
    "        f.col(\"source_name\").alias(\"dim_source_name\"),\n",
    "        \"allocation_date\",\n",
    "        \"otap_capable_flag\",\n",
    "        \"volte_capable_flag\",\n",
    "    )\n",
    "\n",
    "    return df_handset_lookup.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_handset_weekly_data(\n",
    "    df_handset_dd: pyspark.sql.DataFrame,\n",
    "    df_handset_lookup: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates device_dd table to msisdn, weekstart\n",
    "    :param df_handset_dd: Handset daily level data\n",
    "    :param df_handset_lookup: Handset Lookup data\n",
    "    :param is_incremental: Flag for Incremental Approach\n",
    "    :param partition_num: Number of Partition\n",
    "    :return df_handset_weekly: Weekly Aggregated Device data\n",
    "    \"\"\"\n",
    "    df_handset_dd = (\n",
    "        df_handset_dd.join(\n",
    "            df_handset_lookup,\n",
    "            df_handset_dd[\"imei\"].substr(1, 8) == df_handset_lookup[\"tac\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"trx_date\", f.to_date(f.col(\"event_date\").cast(t.StringType()), \"yyyy-MM-dd\")\n",
    "        )\n",
    "        .withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "    )\n",
    "\n",
    "    df_handset_weekly = df_handset_dd.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        f.collect_set(\"manufacturer\").alias(\"manufacturer\"),\n",
    "        f.collect_set(\"imei\").alias(\"imeis\"),\n",
    "        f.collect_set(\"device_type\").alias(\"device_types\"),\n",
    "        f.collect_set(\"market_name\").alias(\"market_names\"),\n",
    "        f.collect_set(\"band\").alias(\"bands\"),\n",
    "        f.collect_set(\"gprs_capable_flag\").alias(\"gprs_capable_flags\"),\n",
    "        f.collect_set(\"edge_capable_flag\").alias(\"edge_capable_flags\"),\n",
    "        f.collect_set(\"umts_capable_flag\").alias(\"umts_capable_flags\"),\n",
    "        f.collect_set(\"hsdpa_capable_flag\").alias(\"hsdpa_capable_flags\"),\n",
    "        f.collect_set(\"lte_capable_flag\").alias(\"lte_capable_flags\"),\n",
    "        f.collect_set(\"highest_network\").alias(\"highest_networks\"),\n",
    "        f.collect_set(\"data_capable_flag\").alias(\"data_capable_flags\"),\n",
    "        f.collect_set(\"wlan_capable_flag\").alias(\"wlan_capable_flags\"),\n",
    "        f.collect_set(\"bluetooth_capable_flag\").alias(\"bluetooth_capable_flags\"),\n",
    "        f.collect_set(\"dual_sim_flag\").alias(\"dual_sim_flags\"),\n",
    "        f.collect_set(\"nfc_capable_flag\").alias(\"nfc_capable_flags\"),\n",
    "        f.collect_set(\"form_factor\").alias(\"form_factors\"),\n",
    "        f.collect_set(\"os_version\").alias(\"os_versions\"),\n",
    "        f.collect_set(\"os_name\").alias(\"os_names\"),\n",
    "        f.collect_set(\"source_name\").alias(\"source_names\"),\n",
    "        f.collect_set(\"allocation_date\").alias(\"allocation_dates\"),\n",
    "        f.collect_set(\"otap_capable_flag\").alias(\"otap_capable_flags\"),\n",
    "        f.collect_set(\"volte_capable_flag\").alias(\"volte_capable_flags\"),\n",
    "    )\n",
    "\n",
    "    return df_handset_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_first_2_handset(\n",
    "    df_handset_mm: pyspark.sql.DataFrame,\n",
    "    df_handset_lookup: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets first and second IMEI per msisdn\n",
    "    :param df_handset_mm: Handset monthly data\n",
    "    :param df_handset_lookup: Lookup static table for each IMEI\n",
    "    :param is_incremental: Flag for Incremental Approach\n",
    "    :param partition_num: Number of Partition\n",
    "    :return df_first_2_handset : first & second IMEI for each msisdn\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch Latest Record for each MSISDN per month\n",
    "    df_handset_mm = df_handset_mm.withColumn(\n",
    "        \"row_number\",\n",
    "        f.row_number().over(\n",
    "            Window.partitionBy(\"msisdn\", \"mm_date\").orderBy(f.col(\"event_date\").desc())\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    df_handset_mm = df_handset_mm.filter(f.col(\"row_number\") == 1).select(\n",
    "        [\"msisdn\", \"mm_date\", \"first_imei\", \"second_imei\"]\n",
    "    )\n",
    "\n",
    "    # Joining lookup table to get manufacturer for First & Second IMEI\n",
    "    df_first_2_handset = df_handset_mm.join(\n",
    "        df_handset_lookup,\n",
    "        df_handset_mm[\"first_imei\"].substr(1, 8) == df_handset_lookup[\"tac\"],\n",
    "        how=\"inner\",\n",
    "    ).select(\n",
    "        \"msisdn\",\n",
    "        \"mm_date\",\n",
    "        f.col(\"manufacturer\").alias(\"first_handset_manufacturer\"),\n",
    "        \"second_imei\",\n",
    "    )\n",
    "\n",
    "    df_first_2_handset = df_first_2_handset.join(\n",
    "        df_handset_lookup,\n",
    "        df_first_2_handset[\"second_imei\"].substr(1, 8) == df_handset_lookup[\"tac\"],\n",
    "        how=\"left\",\n",
    "    ).select(\n",
    "        \"msisdn\",\n",
    "        \"mm_date\",\n",
    "        \"first_handset_manufacturer\",\n",
    "        f.col(\"manufacturer\").alias(\"second_handset_manufacturer\"),\n",
    "    )\n",
    "\n",
    "    df_first_2_handset = df_first_2_handset.withColumn(\n",
    "        \"month\", f.trunc(\"mm_date\", \"month\")\n",
    "    ).select(\n",
    "        \"msisdn\", \"month\", \"first_handset_manufacturer\", \"second_handset_manufacturer\"\n",
    "    )\n",
    "\n",
    "    return df_first_2_handset.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internet Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_abt_format(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "    weekends = ['Sat', 'Sun']\n",
    "\n",
    "    out = (df\n",
    "           .groupBy(\"msisdn\", \"trx_date\", \"access_type\")\n",
    "           .agg(\n",
    "            F.col(\"msisdn\"),\n",
    "            F.col(\"trx_date\"),\n",
    "            F.sum(\"bucket_usage\").alias(\"tot_kb\"),\n",
    "            F.sum(\"trx\").alias(\"tot_trx\"),\n",
    "            F.when(F.col(\"access_type\") == \"2\", F.sum(F.col(\"bucket_usage\"))).otherwise(F.lit(0))\n",
    "                .alias(\"vol_data_2g_kb\"),\n",
    "            F.when(F.col(\"access_type\") == \"1\", F.sum(F.col(\"bucket_usage\"))).otherwise(F.lit(0))\n",
    "                .alias(\"vol_data_3g_kb\"),\n",
    "            F.when(F.col(\"access_type\") == \"6\", F.sum(F.col(\"bucket_usage\"))).otherwise(F.lit(0))\n",
    "                .alias(\"vol_data_4g_kb\")\n",
    "            ).groupBy(\"msisdn\", \"trx_date\").agg(\n",
    "                F.col(\"msisdn\"),\n",
    "                F.col(\"trx_date\"),\n",
    "                F.sum(F.col(\"tot_kb\")).alias(\"tot_kb\"),\n",
    "                F.sum(F.col(\"tot_trx\")).alias(\"tot_trx\"),\n",
    "                F.sum(F.col(\"vol_data_2g_kb\")).alias(\"vol_data_2g_kb\"),\n",
    "                F.sum(F.col(\"vol_data_3g_kb\")).alias(\"vol_data_3g_kb\"),\n",
    "                F.sum(F.col(\"vol_data_4g_kb\")).alias(\"vol_data_4g_kb\"),\n",
    "            )\n",
    "           .withColumn(\"tot_kb_weekday\",\n",
    "                       F.when(F.date_format(F.col(\"trx_date\"), \"E\").isin(weekdays), F.col(\"tot_kb\")).otherwise(\n",
    "                           F.lit(0)))\n",
    "           .withColumn(\"tot_kb_weekend\",\n",
    "                       F.when(F.date_format(F.col(\"trx_date\"), \"E\").isin(weekends), F.col(\"tot_kb\")).otherwise(\n",
    "                           F.lit(0)))\n",
    "           .withColumn(\"weekstart\", next_week_start_day(\"trx_date\"))\n",
    "           )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def calculate_max_day_of_week(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    On a weekly basis, take the max usage day. Then on a rolling window basis (e.g. 90 days), find the day of week which\n",
    "    is maximum among previous N weeks\n",
    "    :param df: input dataframe with rolling aggregations\n",
    "    :return: dataframe with additional column of max day of week usage\n",
    "    \"\"\"\n",
    "\n",
    "    pivot_df = (df\n",
    "                .withColumn(\"day_of_week\", F.date_format(F.col(\"trx_date\"), 'E'))\n",
    "                .groupBy(\"msisdn\", \"weekstart\")\n",
    "                .pivot(\"day_of_week\")\n",
    "                .agg(\n",
    "                    F.coalesce(F.sum(\"tot_kb\"), F.lit(0)).alias(\"daily_sum_tot_kb\")\n",
    "                ))\n",
    "\n",
    "    if not 'Mon' in pivot_df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Mon', F.lit(0))\n",
    "    if not 'Tue' in df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Tue', F.lit(0))\n",
    "    if not 'Wed' in df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Wed', F.lit(0))\n",
    "    if not 'Thu' in pivot_df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Thu', F.lit(0))\n",
    "    if not 'Fri' in pivot_df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Fri', F.lit(0))\n",
    "    if not 'Sat' in pivot_df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Sat', F.lit(0))\n",
    "    if not 'Sun' in pivot_df.columns:\n",
    "        pivot_df = pivot_df.withColumn('Sun', F.lit(0))\n",
    "\n",
    "    argmax_udf = F.udf(lambda m: max(m, key=m.get), t.StringType())\n",
    "    weekday_columns = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "\n",
    "    output_df = (df.join(pivot_df, [\"msisdn\", \"weekstart\"])\n",
    "                 .fillna(0, weekday_columns)\n",
    "                 .withColumn(\"Mon\", F.sum(F.col(\"Mon\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"Tue\", F.sum(F.col(\"Tue\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"Wed\", F.sum(F.col(\"Wed\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"Thu\", F.sum(F.col(\"Thu\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"Fri\", F.sum(F.col(\"Fri\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"Sat\", F.sum(F.col(\"Sat\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"Sun\", F.sum(F.col(\"Sun\")).over(get_rolling_window(90)))\n",
    "                 .withColumn(\"fea_max_usage_day_90d\", F.when(F.col(\"tot_kb\") > 0, argmax_udf(\n",
    "                    F.create_map(list(\n",
    "                        chain(*[(F.lit(c), F.col(c)) for c in weekday_columns]))))\n",
    "                    ).otherwise(None)\n",
    "                 ))\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def week_grouping(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    return df.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        F.col(\"msisdn\"),\n",
    "        F.first(F.col(\"fea_max_usage_day_90d\")).alias(\"fea_int_usage_max_usage_day_90d\"),\n",
    "        F.sum(F.col(\"tot_kb\")).alias(\"tot_kb\"),\n",
    "        F.sum(F.col(\"tot_trx\")).alias(\"tot_trx\"),\n",
    "        F.sum(F.col(\"tot_kb_weekday\")).alias(\"tot_kb_weekday\"),\n",
    "        F.sum(F.col(\"tot_kb_weekend\")).alias(\"tot_kb_weekend\"),\n",
    "        F.sum(F.col(\"vol_data_2g_kb\")).alias(\"vol_data_2g_kb\"),\n",
    "        F.sum(F.col(\"vol_data_3g_kb\")).alias(\"vol_data_3g_kb\"),\n",
    "        F.sum(F.col(\"vol_data_4g_kb\")).alias(\"vol_data_4g_kb\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_weekly_internet_usage_table(\n",
    "    df_internet_usage: pyspark.sql.DataFrame,\n",
    "    partition_num: int\n",
    ") -> pyspark.sql.DataFrame:\n",
    "\n",
    "    preprocessed = preprocess_to_abt_format(df_internet_usage)\n",
    "    preprocessed_dow = calculate_max_day_of_week(preprocessed)\n",
    "    df_internet_usage_grouped = week_grouping(preprocessed_dow)\n",
    "\n",
    "    return df_internet_usage_grouped.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recharge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recharge_weekly(\n",
    "    df_recharge_daily: pyspark.sql.DataFrame, partition_num: int\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    :param df_recharge_daily: Daily Recharge Data\n",
    "    :param is_incremental: Flag for Incremental Approach\n",
    "    :param partition_num: Number of Partition\n",
    "    :return: df_agg: Weekly aggregated recharge table\n",
    "    \"\"\"\n",
    "\n",
    "    df_recharge_daily = df_recharge_daily.withColumn(\n",
    "        \"trx_date\", f.to_date(f.col(\"trx_date\").cast(t.StringType()), \"yyyy-MM-dd\"),\n",
    "    ).withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "\n",
    "    df_recharge_weekly = df_recharge_daily.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        f.sum(\"tot_amt\").alias(\"tot_amt\"), f.sum(\"tot_trx\").alias(\"tot_trx\")\n",
    "    )\n",
    "\n",
    "    return df_recharge_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recharge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weekly_account_balance(\n",
    "    df_bal: pyspark.sql.DataFrame, partition_num: int\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    :param df_bal: Balance Dataframe containing balance at daily level\n",
    "    :param is_incremental: Flag for Incremental Approach\n",
    "    :param partition_num: Number of Partition\n",
    "    :return: Dataframe with the count of Negative or Zero Balance Day count for each MSISDNs on weekly basis\n",
    "    \"\"\"\n",
    "\n",
    "    df_neg_balance = df_bal.withColumn(\n",
    "        \"weekstart\", next_week_start_day(\"event_date\")\n",
    "    ).withColumn(\n",
    "        \"is_zero_or_neg\", F.when((F.col(\"account_balance\") <= 0), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    df_neg_balance_count = df_neg_balance.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        F.sum(\"is_zero_or_neg\").alias(\"zero_or_neg_count\")\n",
    "    )\n",
    "\n",
    "    return df_neg_balance_count.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recharge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_package_purchase_weekly(\n",
    "    df_package_purchase_daily: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    :param df_package_purchase_daily: Package Purchase Daily Data\n",
    "    :param is_incremental: Flag for Incremental Approach\n",
    "    :param partition_num: Number of Partition\n",
    "    :return: SMS, Data & Roaming Recharge Weekly Aggregated Data\n",
    "    \"\"\"\n",
    "    df_pkg_purchase = (\n",
    "        df_package_purchase_daily.withColumn(\n",
    "            \"trx_date\", f.to_date(f.col(\"trx_date\").cast(t.StringType()), \"yyyy-MM-dd\"),\n",
    "        )\n",
    "        .withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "        .select(\n",
    "            \"msisdn\",\n",
    "            \"weekstart\",\n",
    "            \"trx_pkg_prchse\",\n",
    "            \"rev_pkg_prchse\",\n",
    "            \"trx_voice_pkg_prchs\",\n",
    "            \"trx_sms_pkg_prchs\",\n",
    "            \"trx_data_pkg_prchs\",\n",
    "            \"trx_roam_pkg_prchs\",\n",
    "            \"rev_voice_pkg_prchs\",\n",
    "            \"rev_sms_pkg_prchs\",\n",
    "            \"rev_data_pkg_prchs\",\n",
    "            \"rev_roam_pkg_prchs\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_pkg_purchase_weekly = df_pkg_purchase.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        f.sum(\"trx_pkg_prchse\").cast(t.LongType()).alias(\"trx_pkg_prchse\"),\n",
    "        f.sum(\"rev_pkg_prchse\").cast(t.LongType()).alias(\"rev_pkg_prchse\"),\n",
    "        f.sum(\"trx_voice_pkg_prchs\").cast(t.LongType()).alias(\"trx_voice_pkg_prchs\"),\n",
    "        f.sum(\"trx_sms_pkg_prchs\").cast(t.LongType()).alias(\"trx_sms_pkg_prchs\"),\n",
    "        f.sum(\"trx_data_pkg_prchs\").cast(t.LongType()).alias(\"trx_data_pkg_prchs\"),\n",
    "        f.sum(\"trx_roam_pkg_prchs\").cast(t.LongType()).alias(\"trx_roam_pkg_prchs\"),\n",
    "        f.sum(\"rev_voice_pkg_prchs\").cast(t.LongType()).alias(\"rev_voice_pkg_prchs\"),\n",
    "        f.sum(\"rev_sms_pkg_prchs\").cast(t.LongType()).alias(\"rev_sms_pkg_prchs\"),\n",
    "        f.sum(\"rev_data_pkg_prchs\").cast(t.LongType()).alias(\"rev_data_pkg_prchs\"),\n",
    "        f.sum(\"rev_roam_pkg_prchs\").cast(t.LongType()).alias(\"rev_roam_pkg_prchs\"),\n",
    "    )\n",
    "\n",
    "    return df_pkg_purchase_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weekly_revenue_table(\n",
    "    df_rech_mkios_dd: pyspark.sql.DataFrame, df_rech_urp_dd: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"Creates weekly recharge table by:\n",
    "        1. Unions rech_mkios_dd and rech_urp_dd\n",
    "        2. Aggregates tot_trx and tot_amt to weekly level for each msisdn\n",
    "\n",
    "    Args:\n",
    "        df_rech_mkios_dd: rech_mkios_dd daily level table\n",
    "        df_rech_urp_dd: ech_urp_dd daily level table\n",
    "\n",
    "    Returns:\n",
    "        df_agg: Weekly aggregated recharge table\n",
    "    \"\"\"\n",
    "    df_rech_mkios_dd = (\n",
    "        df_rech_mkios_dd.withColumn(\"trx_date\", f.to_date(\"trx_date\", \"yyyyMMdd\"))\n",
    "        .withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "        .withColumn(\"is_weekend\", dayofweek(\"trx_date\").isin([1, 7]).cast(\"int\"))\n",
    "        .select(\n",
    "            \"msisdn\",\n",
    "            \"trx_date\",\n",
    "            \"weekstart\",\n",
    "            \"rev_voice\",\n",
    "            \"rev_data\",\n",
    "            \"rev_sms\",\n",
    "            \"is_weekend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_rech_urp_dd = (\n",
    "        df_rech_urp_dd.withColumn(\"trx_date\", f.to_date(\"trx_date\", \"yyyyMMdd\"))\n",
    "        .withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "        .withColumn(\"is_weekend\", dayofweek(\"trx_date\").isin([1, 7]).cast(\"int\"))\n",
    "        .select(\n",
    "            \"msisdn\",\n",
    "            \"trx_date\",\n",
    "            \"weekstart\",\n",
    "            \"rev_voice_pkg_prchs\",\n",
    "            \"rev_data_pkg_prchs\",\n",
    "            \"rev_sms_pkg_prchs\",\n",
    "            \"is_weekend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = df_rech_mkios_dd.union(df_rech_urp_dd)\n",
    "\n",
    "    df_agg = df.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        f.sum(\"rev_voice\").alias(\"rev_voice\"),\n",
    "        f.sum(\"rev_data\").alias(\"rev_data\"),\n",
    "        f.sum(\"rev_sms\").alias(\"rev_sms\"),\n",
    "        f.sum(f.when(f.col(\"is_weekend\") == 1, f.col(\"rev_voice\"))).alias(\n",
    "            \"rev_voice_weekend\"\n",
    "        ),\n",
    "        f.sum(f.when(f.col(\"is_weekend\") == 1, f.col(\"rev_data\"))).alias(\n",
    "            \"rev_data_weekend\"\n",
    "        ),\n",
    "        f.sum(f.when(f.col(\"is_weekend\") == 1, f.col(\"rev_sms\"))).alias(\n",
    "            \"rev_sms_weekend\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return df_agg.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Messaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_messaging_weekly(\n",
    "    df_abt_usage_mss_dd: pyspark.sql.DataFrame,\n",
    "    partition_num: int\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates Weekly Text Messaging for the partner MSISDNs\n",
    "\n",
    "    :param df_abt_usage_mss_dd: Daily Text Messaging Usage DataFrame\n",
    "\n",
    "    :return: Text Messaging Weekly DataFrame\n",
    "    \"\"\"\n",
    "    SMS_IN_FLAG = \"05_sms_in\"\n",
    "    SMS_OUT_FLAG = \"04_sms_out\"\n",
    "    CALL_IN_FLAG = \"02_call_in\"\n",
    "    CALL_OUT_FLAG = \"01_call_out\"\n",
    "    CALL_TYPE_COL = \"calltype\"\n",
    "\n",
    "    count_cond = lambda cond: F.sum(F.when(cond, F.col(\"total_trx\")).otherwise(0))\n",
    "\n",
    "    df_txt_msg = (\n",
    "        df_abt_usage_mss_dd.withColumn(\"trx_date\", F.to_date(\"trx_date\", \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"weekstart\", next_week_start_day(\"trx_date\"))\n",
    "        .withColumn(\"msisdn\", F.col(\"anumber\"))\n",
    "    )\n",
    "\n",
    "    df_txt_msg_w = df_txt_msg.groupBy(\"msisdn\", \"weekstart\").agg(\n",
    "        count_cond(F.col(CALL_TYPE_COL) == SMS_IN_FLAG).alias(\"count_txt_msg_incoming\"),\n",
    "        count_cond(F.col(CALL_TYPE_COL) == SMS_OUT_FLAG).alias(\n",
    "            \"count_txt_msg_outgoing\"\n",
    "        ),\n",
    "        count_cond(\n",
    "            (F.col(CALL_TYPE_COL) == SMS_OUT_FLAG)\n",
    "            | (F.col(CALL_TYPE_COL) == SMS_IN_FLAG)\n",
    "        ).alias(\"count_txt_msg_all\"),\n",
    "        count_cond(F.col(CALL_TYPE_COL) == CALL_IN_FLAG).alias(\"count_voice_incoming\"),\n",
    "        count_cond(F.col(CALL_TYPE_COL) == CALL_OUT_FLAG).alias(\"count_voice_outgoing\"),\n",
    "        count_cond(\n",
    "            (F.col(CALL_TYPE_COL) == CALL_IN_FLAG)\n",
    "            | (F.col(CALL_TYPE_COL) == CALL_OUT_FLAG)\n",
    "        ).alias(\"count_voice_all\"),\n",
    "    )\n",
    "\n",
    "    return df_txt_msg_w.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commercial Text Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hexa_to_text(df):\n",
    "    df = df.withColumn(\n",
    "        \"bnumber_decoded\",\n",
    "        F.when(\n",
    "            F.length(\"bnumber\") % 2 != 0,\n",
    "            F.expr(\"substring(bnumber, 0, length(bnumber)-1)\"),\n",
    "        ).otherwise(df[\"bnumber\"]),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"bnumber_decoded\",\n",
    "        F.when(\n",
    "            ~df.bnumber.startswith(\"628\"),\n",
    "            F.decode(\n",
    "                F.unhex(F.translate(F.col(\"bnumber_decoded\"), \":;<=>?\", \"ABCDEF\")),\n",
    "                \"US-ASCII\",\n",
    "            ),\n",
    "        ).otherwise(df[\"bnumber\"]),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"bnumber_decoded\",\n",
    "        F.when(F.lower(df[\"bnumber_decoded\"]) == \"krediv\", \"kredivo\").otherwise(\n",
    "            df[\"bnumber_decoded\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_commercial_text_messaging_weekly(\n",
    "    df_abt_usage_mss_dd: pyspark.sql.DataFrame,\n",
    "    df_comm_text_mapping: pyspark.sql.DataFrame,\n",
    "    partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates Weekly Commercial Text Messaging for the partner MSISDNs\n",
    "\n",
    "    :param df_abt_usage_mss_dd: Daily Commercial Text Messaging Usage DataFrame\n",
    "\n",
    "    :param df_comm_text_mapping: Mapping between Sender and Category\n",
    "\n",
    "    :return: Text Messaging Weekly DataFrame\n",
    "    \"\"\"\n",
    "    count_cond = lambda cond: F.sum(F.when(cond, F.col(\"total_trx\")).otherwise(0))\n",
    "\n",
    "    df_abt_usage_mss_dd = (\n",
    "        df_abt_usage_mss_dd.withColumn(\n",
    "            \"weekstart\", next_week_start_day(F.col(\"trx_date\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"trx_date\", F.to_date(F.col(\"trx_date\").cast(t.StringType()), \"yyyyMMdd\")\n",
    "        )\n",
    "        .withColumn(\"msisdn\", F.col(\"anumber\"))\n",
    "    )\n",
    "\n",
    "    df_abt_usage_mss_dd = convert_hexa_to_text(df_abt_usage_mss_dd)\n",
    "\n",
    "    df_comm = df_abt_usage_mss_dd.join(\n",
    "        F.broadcast(df_comm_text_mapping),\n",
    "        (\n",
    "            F.lower(df_abt_usage_mss_dd.bnumber_decoded)\n",
    "            == F.lower(df_comm_text_mapping.sender)\n",
    "        ),\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    df_weekly = df_comm.groupBy(\"msisdn\", \"weekstart\", \"category\").agg(\n",
    "        F.collect_set(\"sender\").alias(\"senders_07d\"),\n",
    "        count_cond((F.col(\"calltype\") == \"05_sms_in\")).alias(\"incoming_count_07d\"),\n",
    "        count_cond(\n",
    "            (\n",
    "                (\n",
    "                    F.col(\"evalueserve_comments\").isin(\n",
    "                        {\n",
    "                            \"bureau of taxation and taxation as a sub-ordinate of the financial administration - public sector\",\n",
    "                            \"tax office\",\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            & (F.col(\"calltype\") == \"05_sms_in\")\n",
    "        ).alias(\"count_txt_msg_incoming_government_tax\"),\n",
    "    )\n",
    "\n",
    "    return df_weekly.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_voice_calling_to_weekly(\n",
    "    df_voice: pyspark.sql.DataFrame, partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregated voice calling data to weekly (msisdn, week)\n",
    "    :param df_voice: Input dataframe for voice calling\n",
    "    :param is_incremental: Flag for Incremental Approach\n",
    "    :param partition_num: Number of Partition\n",
    "    :return df_voice_agg: Weekly aggregated voice calling dataFrame\n",
    "    \"\"\"\n",
    "    if (\"anumber\" in df_voice.columns) and (\"msisdn\" not in df_voice.columns):\n",
    "        df_voice = df_voice.withColumnRenamed(\"anumber\", \"msisdn\")\n",
    "\n",
    "    df_voice_agg = (\n",
    "        df_voice.withColumn(\n",
    "            \"trx_date\", f.to_date(f.col(\"trx_date\").cast(t.StringType()), \"yyyy-MM-dd\"),\n",
    "        )\n",
    "        .withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "        .groupBy(\"msisdn\", \"weekstart\")\n",
    "        .agg(\n",
    "            f.sum(\n",
    "                f.when(\n",
    "                    f.col(\"calltype\") == \"02_call_in\", f.col(\"total_duration\")\n",
    "                ).otherwise(0)\n",
    "            ).alias(\"inc_call_dur\"),\n",
    "            f.sum(\n",
    "                f.when(\n",
    "                    f.col(\"calltype\") == \"01_call_out\", f.col(\"total_duration\")\n",
    "                ).otherwise(0)\n",
    "            ).alias(\"out_call_dur\"),\n",
    "            f.sum(f.col(\"duration_early_morning\")).alias(\"total_dur_12_5_am\"),\n",
    "            f.collect_list(\n",
    "                f.when(f.col(\"calltype\") == \"02_call_in\", f.col(\"bnumber\"))\n",
    "            ).alias(\"in_call_nums\"),\n",
    "            f.collect_list(\n",
    "                f.when(f.col(\"calltype\") == \"01_call_out\", f.col(\"bnumber\"))\n",
    "            ).alias(\"out_call_nums\"),\n",
    "            f.collect_list(\n",
    "                f.when(\n",
    "                    f.col(\"calltype\").isin([\"02_call_in\", \"01_call_out\"]),\n",
    "                    f.col(\"bnumber\"),\n",
    "                )\n",
    "            ).alias(\"inc_out_call_nums\"),\n",
    "        )\n",
    "        .select(\n",
    "            \"msisdn\",\n",
    "            \"weekstart\",\n",
    "            \"inc_call_dur\",\n",
    "            \"out_call_dur\",\n",
    "            \"total_dur_12_5_am\",\n",
    "            \"in_call_nums\",\n",
    "            \"out_call_nums\",\n",
    "            \"inc_out_call_nums\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_voice_agg.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Calls Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_voice_calling_to_weekly_old(\n",
    "    df_voice: pyspark.sql.DataFrame, partition_num: int,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregated voice calling data to weekly (msisdn, week)\n",
    "    :param df_voice: Input dataframe for voice calling\n",
    "    :param partition_num: Number of Partition\n",
    "    :return df_voice_agg: Weekly aggregated voice calling dataFrame\n",
    "    \"\"\"\n",
    "    if (\"anumber\" in df_voice.columns) and (\"msisdn\" not in df_voice.columns):\n",
    "        df_voice = df_voice.withColumnRenamed(\"anumber\", \"msisdn\")\n",
    "\n",
    "    df_voice_agg = (\n",
    "        df_voice.withColumn(\n",
    "            \"trx_date\", f.to_date(f.col(\"day\").cast(t.StringType()), \"yyyyMMdd\"),\n",
    "        )\n",
    "        .withColumn(\"weekstart\", next_week_start_day(f.col(\"trx_date\")))\n",
    "        .groupBy(\"msisdn\", \"weekstart\")\n",
    "        .agg(\n",
    "            f.sum(\n",
    "                f.when(\n",
    "                    f.col(\"calltype\") == \"02_call_in\", f.col(\"total_duration\")\n",
    "                ).otherwise(0)\n",
    "            ).alias(\"inc_call_dur\"),\n",
    "            f.sum(\n",
    "                f.when(\n",
    "                    f.col(\"calltype\") == \"01_call_out\", f.col(\"total_duration\")\n",
    "                ).otherwise(0)\n",
    "            ).alias(\"out_call_dur\"),\n",
    "            f.sum(f.col(\"duration_early_morning\")).alias(\"total_dur_12_5_am\"),\n",
    "            f.collect_list(\n",
    "                f.when(f.col(\"calltype\") == \"02_call_in\", f.col(\"bnumber\"))\n",
    "            ).alias(\"in_call_nums\"),\n",
    "            f.collect_list(\n",
    "                f.when(f.col(\"calltype\") == \"01_call_out\", f.col(\"bnumber\"))\n",
    "            ).alias(\"out_call_nums\"),\n",
    "            f.collect_list(\n",
    "                f.when(\n",
    "                    f.col(\"calltype\").isin([\"02_call_in\", \"01_call_out\"]),\n",
    "                    f.col(\"bnumber\"),\n",
    "                )\n",
    "            ).alias(\"inc_out_call_nums\"),\n",
    "        )\n",
    "        .select(\n",
    "            \"msisdn\",\n",
    "            \"inc_call_dur\",\n",
    "            \"out_call_dur\",\n",
    "            \"total_dur_12_5_am\",\n",
    "            \"in_call_nums\",\n",
    "            \"out_call_nums\",\n",
    "            \"inc_out_call_nums\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_voice_agg.repartition(numPartitions=partition_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_aggregation(pipeline, start_date='20190101', end_date=date.today().strftime(\"%Y%m%d\"), date_col_format='%Y-%m-%d'):\n",
    "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    start_date = start_date - timedelta(days=start_date.weekday())\n",
    "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
    "    end_date = end_date + timedelta(days=(7 - end_date.weekday()))\n",
    "\n",
    "    while start_date < end_date:\n",
    "        dates = []\n",
    "        for i in range(7):\n",
    "            dates.append(start_date.strftime(date_col_format))\n",
    "            start_date += timedelta(days=1)\n",
    "        weekstart = start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        print(dates)\n",
    "        if 'bcp-1' == pipeline:\n",
    "            bcp_weekly_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/internet_apps_usage/bcp_usage_weekly.parquet\"\n",
    "            df = spark.read.table(\"mig.smy_bcp_usage_dd\").filter(f.col(\"trx_date\").isin(dates))\n",
    "            week_df = create_bcp_usage_weekly(df, partition_size_5)\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(bcp_weekly_path)\n",
    "        elif 'bcp-2' == pipeline:\n",
    "            bcp_weekly_path_1 = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/internet_apps_usage/bcp_usage_weekly.parquet/weekstart={weekstart}\"\n",
    "            bcp_feature_mapping_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/internet_apps_usage/bcp_feature_category_mapping.parquet\"\n",
    "            bcp_weekly_path_2 = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/internet_apps_usage/bcp_feature_weekly_aggregation.parquet\"\n",
    "            df_bcp_feature_mapping = spark.read.parquet(bcp_feature_mapping_path)\n",
    "            df = spark.read.parquet(bcp_weekly_path_1.format(weekstart=weekstart))\n",
    "            df = df.withColumn(\"weekstart\", f.lit(weekstart))\n",
    "            week_df = create_bcp_weekly_feature_aggregation(df, df_bcp_feature_mapping, partition_size_5)\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(bcp_weekly_path_2)\n",
    "        elif \"cust_prof\" == pipeline:\n",
    "            print(\"weekstart={weekstart}\".format(weekstart=weekstart))\n",
    "            print(\"event_date={event_date}\".format(event_date=dates[0]))\n",
    "            df = spark.read.table(\"cb.cb_multidim\").filter(f.col(\"event_date\").isin(dates[0]))\n",
    "            week_df = agg_customer_profile_to_weekly(df, partition_size_3)\n",
    "            cust_prof_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/customer_profile/customer_profile_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(cust_prof_path)\n",
    "        elif 'handset-1' == pipeline:\n",
    "            mapp_data = spark.read.table(\"dim.device_dim\")\n",
    "            mapp_df = create_handset_lookup_data(mapp_data, partition_size_0)\n",
    "            mapp_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/handset/device_dim_single_rec_per_imei.parquet\"\n",
    "            mapp_df.write.mode('overwrite').parquet(mapp_path)\n",
    "            df = spark.read.table(\"smy.device_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "            mapp_df = spark.read.parquet(mapp_path)\n",
    "            week_df = create_handset_weekly_data(df, mapp_df, partition_size_5)\n",
    "            handset_1_weekly_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/handset/device_data_weekly_2.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(handset_1_weekly_path)\n",
    "        elif \"internet_usage\" == pipeline:\n",
    "            df = spark.read.table(\"smy.usage_upcc_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "            week_df = preprocess_to_abt_format(df, partition_size_4)\n",
    "            int_usage_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/internet_usage/internet_usage_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(int_usage_path)\n",
    "        elif 'recharge-1' == pipeline:\n",
    "            df = spark.read.table(\"abt.rech_daily_abt_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "            week_df = create_recharge_weekly(df, partition_size_3)\n",
    "            recharge_1_weekly_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/recharge/rech_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(recharge_1_weekly_path)\n",
    "        elif 'recharge-2' == pipeline:\n",
    "            df = spark.read.table(\"base.ocs_bal\").filter(f.col(\"event_date\").isin(dates))\n",
    "            week_df = create_weekly_account_balance(df, partition_size_5)\n",
    "            recharge_2_weekly_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/recharge/acc_bal_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(recharge_2_weekly_path)\n",
    "        elif 'recharge-3' == pipeline:\n",
    "            df = spark.read.table(\"abt.usage_chg_pkg_prchse_abt_dd\").filter(f.col(\"trx_date\") >= \"2019-06-03\")\n",
    "            week_df = create_package_purchase_weekly(df, partition_size_2)\n",
    "            recharge_3_weekly_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/recharge/chg_pkg_prchse_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(recharge_3_weekly_path)\n",
    "        elif \"revenue\" == pipeline:\n",
    "            df = spark.read.table(\"smy.usage_upcc_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "            week_df = create_weekly_revenue_table(df, partition_size_5)\n",
    "            int_usage_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/revenue/revenue_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(int_usage_path)\n",
    "        elif \"sms-1\" == pipeline:\n",
    "            df = spark.read.table(\"abt.usage_mss_abt_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "            week_df = create_text_messaging_weekly(df, partition_size_1)\n",
    "            int_usage_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/sms/text_messaging_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(int_usage_path)\n",
    "        elif \"sms-2\" == pipeline:\n",
    "            df = spark.read.table(\"abt.usage_mss_abt_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "            mapp_df = spark.read.parquet(\"hdfs:///data/landing/gx_pnt/mck_dmp_common/kedro/02_primary/text_messaging_mapping/sms_sender_mapping.parquet\")\n",
    "            week_df = create_commercial_text_messaging_weekly(df, mapp_df, partition_size_1)\n",
    "            int_usage_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/sms/commercial_text_messaging_weekly.parquet\"\n",
    "            week_df.write.partitionBy('weekstart').mode('overwrite').parquet(int_usage_path)\n",
    "        elif 'voice-new' == pipeline:\n",
    "            if weekstart > '2019-05-27'\n",
    "                df = spark.read.table(\"abt.usage_mss_abt_dd\").filter(f.col(\"event_date\").isin(dates))\n",
    "                week_df = aggregate_voice_calling_to_weekly(df, partition_size_3)\n",
    "                voice_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/voice_calls/voice_calls_weekly_agg_2.parquet\"\n",
    "                week_df.write.partitionBy('weekstart').mode('overwrite').parquet(voice_path)\n",
    "        elif 'voice-old' == pipeline:\n",
    "            if weekstart <= '2019-05-27'\n",
    "                df = spark.read.table(\"mck.t_mss_call_full_hist\").filter(f.col(date_col).isin(int(dates[0]), int(dates[1]), int(dates[2]), int(dates[3]), int(dates[4]), int(dates[5]), int(dates[6])))\n",
    "                week_df = aggregate_voice_calling_to_weekly(df, partition_size_3)\n",
    "                voice_path = \"hdfs:///data/landing/gx_pnt/mck_dmp_pipeline/01_aggregation/voice_calls/voice_calls_weekly_agg_2.parquet\"\n",
    "                week_df.write.partitionBy('weekstart').mode('overwrite').parquet(voice_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Aggregation Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weekly_aggregation(\"cust_prof\", start_date=\"20200203\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"handset-1\", start_date=\"20200120\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"bcp\", start_date=\"20200127\", date_col_format='%Y%m%d')\n",
    "# weekly_aggregation(\"internet_usage\", start_date=\"20200113\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"recharge-1\", start_date=\"20200127\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"recharge-2\", start_date=\"20200127\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"recharge-3\", start_date=\"20190603\", end_date=\"20190604\", date_col_format='%Y-%m-%d')\n",
    "\n",
    "# voice_old_weekly_aggregation(start_date='20190128', end_date='20190203')\n",
    "# voice_old_weekly_aggregation(start_date='20190121', end_date='20190127')\n",
    "# voice_old_weekly_aggregation(start_date='20190114', end_date='20190120')\n",
    "\n",
    "# weekly_aggregation(\"sms-1\", start_date=\"20200203\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"sms-2\", start_date=\"20200203\", date_col_format='%Y-%m-%d')\n",
    "\n",
    "# weekly_aggregation(\"bcp-2\", start_date=\"20200127\", end_date=\"20200202\", date_col_format='%Y%m%d')\n",
    "# weekly_aggregation(\"bcp-1\", start_date=\"20200203\", date_col_format='%Y%m%d')\n",
    "# weekly_aggregation(\"bcp-2\", start_date=\"20200203\", date_col_format='%Y-%m-%d')\n",
    "\n",
    "# weekly_aggregation(\"handset-1\", start_date=\"20200120\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"handset-1\", start_date=\"20190101\", end_date=\"20190203\", date_col_format='%Y-%m-%d')\n",
    "\n",
    "# weekly_aggregation(\"cust_prof\", start_date=\"20200203\", end_date=\"20200215\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"cust_prof\", start_date=\"20190401\", end_date=\"20190526\", date_col_format='%Y-%m-%d')\n",
    "\n",
    "# weekly_aggregation(\"internet_usage\", start_date=\"20200113\", date_col_format='%Y-%m-%d')\n",
    "\n",
    "# weekly_aggregation(\"recharge-1\", start_date=\"20190101\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"recharge-2\", start_date=\"20200127\", date_col_format='%Y-%m-%d')\n",
    "# weekly_aggregation(\"recharge-3\", start_date=\"20200203\", date_col_format='%Y-%m-%d')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
