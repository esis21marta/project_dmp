{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uplift Models for Promoters Campaigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "During the Promoters Campaigns, the effect on monthly cashflow of sending promoters to an outlet may vary in a range from very negative to very positive. A negative effect implies that money was spent in sending the promoters but actually the monthly sales diminished, which is not desirable for Tsel or for the outlet. So we want to send promoters only to outlets where we expect the outcome will be at least enough positive to justify the investment.\n",
    "\n",
    "Uplift models can be used to target the best outlets to send the promoters. In this notebook, we will load the data from the Promoters Pilot to make an uplift model. The model can be used to identify the best outlets to send promoters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is based on the \"causalml\" library from Uber and the data processing is based on typical data science libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from causalml.dataset import synthetic_data\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score \n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Promoters Pilot table and organice it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains the cashflows from february and march for the Promoters Pilot Test and Control Groups.\n",
    "\n",
    "The Control Group was created by finding for each Test outlet, another random outlet having the same classification, type and region. This means both groups have the same size (but we will see later that we don't have features for all outlets, so the sizes will become different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table\n",
    "PP_experiment_df = pd.read_csv('Promoters Pilot input table v2.txt')\n",
    "PP_experiment_df.rename(columns={'Outlet_id':'outlet_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target variable for uplift model\n",
    "PP_experiment_df['Target_variable'] = PP_experiment_df['Cashflow_march_1_29'] # Two options: 'Cashflow_march_1_29' and 'Delta_feb_mar'\n",
    "PP_experiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of test and control outlets\n",
    "outlets_list = PP_experiment_df['outlet_id'].tolist()\n",
    "len(outlets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check stats of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = PP_experiment_df.loc[PP_experiment_df['Treatment'] == 1]\n",
    "test['Target_variable'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = PP_experiment_df.loc[PP_experiment_df['Treatment'] == 0]\n",
    "control['Target_variable'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load outlets master table of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is used as the input for the Reseller's Model, but we will use it to add features to the Test and Control groups outlets. The features will be useful during the modelling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('../../../data/reseller/07_model_output/gridsearchcv/ra_mck_int_gridsearchcv_master_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_df.head()\n",
    "len(master_df)\n",
    "len(master_df.outlet_id.unique())\n",
    "len(master_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many Test and Control outlets in master table of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not at outlets in the Test and Control groups are present in the master table, so we will check how many there are. Note that this causes the Test and Control groups to have smaller and different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of all outlets in master table of features\n",
    "master_outlets_list = master_df.outlet_id.unique().tolist()\n",
    "len(master_outlets_list) # Outlets in master table of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many Test and Control outlets in master table\n",
    "Exp_outlets_in_master_df = PP_experiment_df[PP_experiment_df['outlet_id'].isin(master_outlets_list)]\n",
    "len(Exp_outlets_in_master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for only Test outlets\n",
    "Exp_outlets_in_master_1_df = Exp_outlets_in_master_df.loc[Exp_outlets_in_master_df['Treatment'] == 1]\n",
    "len(Exp_outlets_in_master_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for only Control outlets\n",
    "Exp_outlets_in_master_0_df = Exp_outlets_in_master_df.loc[Exp_outlets_in_master_df['Treatment'] == 0]\n",
    "len(Exp_outlets_in_master_0_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check target variable stats after filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to calculate stats again after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target variable average of Test outlets\n",
    "y_test = Exp_outlets_in_master_1_df.loc[Exp_outlets_in_master_1_df['Treatment'] == 1]\n",
    "y_test['Target_variable'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target variable average of Control outlets\n",
    "y_control = Exp_outlets_in_master_0_df.loc[Exp_outlets_in_master_0_df['Treatment'] == 0]\n",
    "y_control['Target_variable'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter master table of features by outlets list (Test and Control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter the master table of features by the Test and Control outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_features_df = master_df[master_df['outlet_id'].isin(outlets_list)]\n",
    "len(PP_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of columns\n",
    "len(PP_features_df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NA columns just in case\n",
    "PP_features_df = PP_features_df.dropna(axis=1)\n",
    "len(PP_features_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach master table of features to Promoters Pilot table\n",
    "\n",
    "Now both tables are joined to produce a Promoters Pilot master table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_master_df = PP_experiment_df.join(PP_features_df.set_index('outlet_id'), on='outlet_id')\n",
    "#PP_master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_master_df = PP_master_df.dropna(axis=0)\n",
    "len(PP_master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PP_master_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target variable, features and treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the target variable, features and treatment. These will be used not just for the uplift models, but for two previous models we have to do to select main features and check sampling bias in the Test and Control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create target variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = PP_master_df['Target_variable']\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = PP_master_df['Treatment']\n",
    "len(treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder1= LabelEncoder() #initializing an object of class LabelEncoder\n",
    "PP_master_df['Type'] = labelencoder1.fit_transform(PP_master_df['Type'])\n",
    "\n",
    "labelencoder2= LabelEncoder() #initializing an object of class LabelEncoder\n",
    "PP_master_df['Classification'] = labelencoder2.fit_transform(PP_master_df['Classification'])\n",
    "\n",
    "labelencoder3= LabelEncoder() #initializing an object of class LabelEncoder\n",
    "PP_master_df['Region'] = labelencoder3.fit_transform(PP_master_df['Region'])\n",
    "\n",
    "PP_master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for relevant columns (*please edit the indices)\n",
    "PP_master_df.columns[0:20].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant columns\n",
    "df1 = PP_master_df.iloc[:,3:6]\n",
    "df2 = PP_master_df.iloc[:,10:11]\n",
    "df3 = PP_master_df.iloc[:,13:2109]\n",
    "df4 = PP_master_df.iloc[:,2214:2372]\n",
    "x = pd.concat([df1,df2,df3,df4], axis=1)\n",
    "len(x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only float and int columns\n",
    "columns = []\n",
    "for j in range(0,len(x.columns)):\n",
    "    if x.dtypes[j] == 'float64' or x.dtypes[j] == 'int64':\n",
    "        columns.append(x.columns[j])\n",
    "x = x[columns]\n",
    "print(len(x.columns)) # In the next section the number of features will be reduces to 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest model to define subset of x features to use in uplift model\n",
    "\n",
    "We create this random forest classifier model to select the top features to use in the uplift model. This will modify the number of features in dataframe \"x\" which we will use for the uplift model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create target variable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify target variable in 4 categories corresponding to quartiles, and use categories as as target\n",
    "# variable only for this model\n",
    "PP_master_df['Target_variable'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash = PP_master_df['Target_variable'].tolist()\n",
    "PP_master_df['target_class1'] = -1\n",
    "target1 = PP_master_df['target_class1'].tolist()\n",
    "for i in range(0,len(cash)):\n",
    "    if cash[i] > np.percentile(PP_master_df.Target_variable, 75):\n",
    "        target1[i] = 4\n",
    "    elif cash[i] <= np.percentile(PP_master_df.Target_variable, 75) and cash[i] > np.percentile(PP_master_df.Target_variable, 50):\n",
    "        target1[i] = 3\n",
    "    elif cash[i] <= np.percentile(PP_master_df.Target_variable, 50) and cash[i] > np.percentile(PP_master_df.Target_variable, 25):  \n",
    "        target1[i] = 2\n",
    "    elif cash[i] <= np.percentile(PP_master_df.Target_variable, 25):\n",
    "        target1[i] = 1\n",
    "    else:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target variable\n",
    "PP_master_df['target_class1'] = target1\n",
    "y_class = PP_master_df['target_class1']\n",
    "len(y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_class, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestClassifier()\n",
    "regressor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chooose top variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = pd.Series(x.columns)\n",
    "Feature_importances = pd.Series(regressor.feature_importances_)\n",
    "Feature_importances_dic = {'Variable': Variables, \"Feature_importance\": Feature_importances}\n",
    "Feature_importances_df = pd.DataFrame(Feature_importances_dic)\n",
    "fis = Feature_importances_df.sort_values(by=\"Feature_importance\", ascending=False)\n",
    "fis.iloc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ranked = fis['Variable'].tolist()\n",
    "top_vars = vars_ranked[0:50]\n",
    "X = x[top_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for possible sample bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the uplift modelling to work properly, a good practice is to create Test and Control groups from random samples. Here we will check if there seems to be a sample bias. For this purpose, we will create a random forest classifier to try to predict the Treatment variable as the target variable. The prediction results must be low to guarantee that there is not an important bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_check_bias = x.drop(columns=['Promoter_days'])\n",
    "x_train, x_test, t_train, t_test = train_test_split(x_check_bias, treatment, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestClassifier()\n",
    "regressor.fit(x_train,t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(t_test,t_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(t_test,t_pred)) # High values might imply bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(t_test,t_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variables = pd.Series(x.columns)\n",
    "Feature_importances = pd.Series(regressor.feature_importances_)\n",
    "Feature_importances_dic = {'Variable': Variables, \"Feature_importance\": Feature_importances}\n",
    "Feature_importances_df = pd.DataFrame(Feature_importances_dic)\n",
    "fis = Feature_importances_df.sort_values(by=\"Feature_importance\", ascending=False)\n",
    "fis.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uplift modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll proceed with creating an Uplift model for the target variable given the promoters presence or absence in the outlets.\n",
    "\n",
    "We use a T-learner from the meta-learner based methods. A meta-learner is a framework to estimate the Conditional Average Treatment effect (CATE). The T-learner makes a model for the Test Group, then another model for the Control Group, and uses the differences (i.e. Test - Control) to estimate uplift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.meta import BaseTRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from causalml.inference.meta import XGBTRegressor\n",
    "from causalml.metrics import plot\n",
    "\n",
    "# Define target variable\n",
    "y = PP_master_df['Target_variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.DataFrame({\"y\": y, \"treatment\": treatment}),\n",
    "    pd.DataFrame(X)],\n",
    "    axis = 1\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check target variable stats for Test and Control groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.loc[data['treatment'] == 1]\n",
    "data_test['y'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.loc[data['treatment'] == 0]\n",
    "data_test['y'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, treatment_train, treatment_test = train_test_split(X, y, treatment, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training T-learner on train\n",
    "learner_t = XGBTRegressor(learner=XGBRegressor(random_state=42))\n",
    "learner_t.fit(X=X_train, treatment=treatment_train, y=y_train)\n",
    "\n",
    "## Get predictions, on the test set\n",
    "uplift, outcome_c, outcome_t = learner_t.predict(X=X_test, return_components=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregating everything on a dataframe\n",
    "df = pd.DataFrame({'y': y_test,\n",
    "                   'w': treatment_test,\n",
    "                   'T-Learner': uplift.reshape(-1)\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qini plot (where uplift is in y-axis, which is average test - average control for for top p population)\n",
    "plot(df,kind='qini', outcome_col='y', treatment_col='w',figsize=(10, 3.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qini score\n",
    "from causalml.metrics import auuc_score, qini_score\n",
    "print('\\nQINI Score\\n',qini_score(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUUC\n",
    "print('AUUC:\\n',auuc_score(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP values can be used to understand the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances using SHAP\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Raw SHAP values\n",
    "shap_values = learner_t.get_shap_values(X=X_test,\n",
    "                                        tau=learner_t.predict(X_test),\n",
    "                                        #we may specify the exact model to be used as additonal one\n",
    "                                        model_tau_feature = RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_values.get(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHAP importance plot will show us the following:\n",
    "\n",
    "- Feature importance: Variables are ranked in descending order.\n",
    "- Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP importance plot\n",
    "learner_t.plot_shap_values(X=X_test, tau=learner_t.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following table matches the SHAP importance plot to the feature names\n",
    "feature_importance = pd.DataFrame({'Feature': X_test.columns,\n",
    "                   'Number': range(0, len(X_test.columns))\n",
    "                  })\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model ready, uplift predictions can be made over any set of outlets (even non-random sets). We will use the outlets from X_test as a didactic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions, on the test set\n",
    "uplift, outcome_c, outcome_t = learner_t.predict(X=X_test, return_components=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_t.get(1) - outcome_c.get(1) # This is equal to the uplift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the histogram of the uplift, we can observe more than half of the outlets have a positive uplift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the uplift\n",
    "plt.hist(uplift, bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the cost of sending a promoter is nearly 3 million IDR + any Tsel admin cost. We will then look for outlets with more than 4 million IDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoters_threshold = 4000000\n",
    "selected_outlets = uplift[uplift>promoters_threshold]\n",
    "len(selected_outlets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68 out of 130 outlets have expected uplifts of more than 4 million IDR. These are ideal outlets to send promoters.\n",
    "\n",
    "The net gain of sending promoters to those outlets is 477 million IDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net_gain = sum(selected_outlets) - len(selected_outlets)*promoters_threshold\n",
    "Net_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Test Group seems to be biased. A model must be built preferrably with an un-biased Test Group.\n",
    "2. Play with different random splits of the train and test sets in all models to check how stable they are.\n",
    "3. Causalml has two types of algorithms: A. Meta-learner algorithms like the one we are using here and B. Tree-based algorithms. Option B can be tested to see if it delivers better results. Some tests are made in the Appendix but with some technical issues to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPENDIX: Other uplift models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassign column names to features dataframe X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_col_names = X\n",
    "X_new_col_names.columns = [\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\",\n",
    "                           \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\",\n",
    "                           \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\",\n",
    "                           \"f30\", \"f31\", \"f32\", \"f33\", \"f34\", \"f35\", \"f36\", \"f37\", \"f38\", \"f39\",\n",
    "                           \"f40\", \"f41\", \"f42\", \"f43\", \"f44\", \"f45\", \"f46\", \"f47\", \"f48\", \"f49\"]\n",
    "X_new_col_names.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, treatment_train, treatment_test, outcome_train, outcome_test = train_test_split(\n",
    "        XX, treatment, y, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta-learner: t-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.meta import BaseTClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tlearner = BaseTClassifier(learner=XGBClassifier(random_state=42, n_estimators = 300,\n",
    "                                    max_depth = 5, learning_rate = 0.1))\n",
    "\n",
    "xgb_tlearner.fit(X=x_train, y=outcome_train, treatment=treatment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.metrics import plot\n",
    "\n",
    "t_pred = xgb_tlearner.predict(X=x_test)\n",
    "\n",
    "## Aggregating everything on a dataframe\n",
    "valid_t = pd.DataFrame({'y': outcome_test,\n",
    "                   'w': treatment_test,\n",
    "                   'T-Learner': t_pred.reshape(-1), \n",
    "                  })\n",
    "\n",
    "## Plotting the 3 types of uplift curve. \n",
    "plot(valid_t,kind='qini', outcome_col='y', treatment_col='w',figsize=(10, 3.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.metrics import auuc_score, qini_score\n",
    "print('AUUC:\\n',auuc_score(valid_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uplift trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.tree import UpliftTreeClassifier\n",
    "from causalml.inference.tree import uplift_tree_string, uplift_tree_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift_tree = UpliftTreeClassifier(max_depth=5, min_samples_leaf=20, min_samples_treatment=5,\n",
    "                                    n_reg=100, evaluationFunction='KL', control_name='control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift_tree.fit(x_train,np.where(treatment_train<1, \"control\", \"treatment\"), y=outcome_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random uplift forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.tree import UpliftRandomForestClassifier\n",
    "from causalml.inference.tree import uplift_tree_string, uplift_tree_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift_forest = UpliftRandomForestClassifier(control_name=\"control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift_forest.fit(x_train,\n",
    "                  np.where(treatment_train<1, \"control\", \"treatment\"),\n",
    "                  y=outcome_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
