{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import types, functions as F\n",
    "\n",
    "# random seed for reproducibility\n",
    "RANDOM_SEED = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'reseller_pilot_sampling'\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster('yarn-client')\n",
    "         .setAppName(app_name)\n",
    "         .set(\"spark.yarn.queue\", \"root.hue_dmp\")\n",
    "         .set(\"spark.executor.memory\", \"16G\")\n",
    "         .set(\"spark.executor.cores\",\"4\")\n",
    "         .set(\"spark.driver.memory\", \"16G\")\n",
    "         .set(\"spark.default.parallelism\", \"8\")\n",
    "         .set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "         .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "         .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "         .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "         .set(\"spark.dynamicAllocation.maxExecutors\", \"20\")\n",
    "         .set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "         .set(\"spark.yarn.maxAppAttempts\", \"2\")\n",
    "         .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "         .set(\"spark.sql.parquet.binaryAsString\", \"true\")\n",
    "         .set(\"spark.driver.memoryOverhead\", \"4096\")\n",
    "         .set(\"spark.yarn.driver.memoryOverhead\", \"4096\")\n",
    "         .set(\"spark.yarn.executor.memoryOverhead\", \"4096\")\n",
    "         .set(\"spark.executor.heartbeatInterval\", \"20s\")\n",
    "         .set(\"spark.network.timeout\", \"800s\")\n",
    "         .set(\"spark.sql.broadcastTimeout\", \"1200\")\n",
    "         .set(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n",
    "         .set(\"yarn.nodemanager.vmem-check-enabled\", \"false\")\n",
    "         .set(\"spark.default.parallelism\", \"8\")\n",
    "         .set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "         .set(\"spark.driver.memory\", \"16G\")\n",
    "         .set(\"spark.testing.memory\", \"2147480000\")\n",
    "         .set(\"spark.sql.hive.verifyPartitionPath\", \"false\")\n",
    "         .set(\"spark.driver.maxResultSize\", \"0\")\n",
    "         .set(\"spark.sql.autoBroadcastJoinThreshold\", 400*1024*1024)\n",
    "         .set(\"yarn.nodemanager.vmem-check-enabled\",\"false\")\n",
    "         .set(\"spark.hadoop.fs.permissions.umask-mode\",\"002\")\n",
    ")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load performance percentiles for outlet in March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filepath = '/home/cdsw/project_mck_dmp/data/reseller/07_model_output/kfold/performance/ra_mck_int_kfold_performance_percentiles_march2020.csv'\n",
    "march_perf = pd.read_csv(csv_filepath).set_index('outlet_id')\n",
    "print(march_perf.shape)\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_perf.training_ground_truths_list_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join with master table to filter to get region, pv columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['outlet_id', 'fea_region', 'fea_outlets_pv_red_sum_nominal_by_rs_msisdn_sum']\n",
    "csv_filepath = '/home/cdsw/project_mck_dmp/data/reseller/05_model_input/master_table/ra_master_table_mar2020.csv'\n",
    "master_table = pd.read_csv(csv_filepath)[cols].set_index('outlet_id')\n",
    "print(master_table.shape)\n",
    "master_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with performance percentile df\n",
    "march_perf = march_perf.join(master_table, how='left')\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillna and remove outlets with physical voucher transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_perf = march_perf.fillna(value={'fea_outlets_pv_red_sum_nominal_by_rs_msisdn_sum': 0.0})\n",
    "print(march_perf.shape)\n",
    "march_perf = march_perf[march_perf.fea_outlets_pv_red_sum_nominal_by_rs_msisdn_sum == 0.0]\n",
    "print(march_perf.shape)\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load May 2020 master table to get cashflow to calculate deviation %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['outlet_id', 'fea_outlet_decimal_total_cashflow_mkios_pv_mean', 'fea_outlets_pv_red_sum_nominal_by_rs_msisdn_sum']\n",
    "csv_filepath = '/home/cdsw/project_mck_dmp/data/reseller/05_model_input/master_table/ra_master_table_may2020.csv'\n",
    "may_cashflows = pd.read_csv(csv_filepath)[cols].set_index('outlet_id')\n",
    "may_cashflows = may_cashflows.fillna({'fea_outlet_decimal_total_cashflow_mkios_pv_mean': 0.0, 'fea_outlets_pv_red_sum_nominal_by_rs_msisdn_sum': 0.0}).rename({'fea_outlet_decimal_total_cashflow_mkios_pv_mean': 'may_2020_cashflow_mkios_pv_mean', 'fea_outlets_pv_red_sum_nominal_by_rs_msisdn_sum': 'physical_voucher_sum_may2020'}, axis=1)\n",
    "print(may_cashflows.shape)\n",
    "may_cashflows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with march_perf\n",
    "march_perf = march_perf.join(may_cashflows, how='left')\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out outlets with physical voucher transactions in May 2020\n",
    "print(march_perf.shape)\n",
    "march_perf = march_perf[march_perf.physical_voucher_sum_may2020 == 0.0]\n",
    "print(march_perf.shape)\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage deviation\n",
    "march_perf['cashflow_percentage_change_absolute'] = ((march_perf['may_2020_cashflow_mkios_pv_mean'] - march_perf['ground_truth']) * 100 / march_perf['ground_truth']).abs()\n",
    "print(march_perf.shape)\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_perf = march_perf[(((march_perf['ground_truth'] >= 400000) & (march_perf['ground_truth'] <= 600000)) | ((march_perf['ground_truth'] >= 240000) & (march_perf['ground_truth'] <= 360000)) | ((march_perf['ground_truth'] >= 80000) & (march_perf['ground_truth'] <= 120000))) & (march_perf['cashflow_percentage_change_absolute'] <= 10)]\n",
    "print(march_perf.shape)\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate percentile cashflows and corresponding differences with actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march_perf['training_ground_truths_list'] = march_perf['training_ground_truths_list'].map(literal_eval)\n",
    "\n",
    "# calculate percentiles cashflows\n",
    "march_perf['75th_percentile_cashflow'] = march_perf['training_ground_truths_list'].map(lambda lst: np.quantile(lst, 0.75))\n",
    "march_perf['90th_percentile_cashflow'] = march_perf['training_ground_truths_list'].map(lambda lst: np.quantile(lst, 0.9))\n",
    "march_perf['95th_percentile_cashflow'] = march_perf['training_ground_truths_list'].map(lambda lst: np.quantile(lst, 0.95))\n",
    "march_perf['100th_percentile_cashflow'] = march_perf['training_ground_truths_list'].map(lambda lst: np.quantile(lst, 1.0))\n",
    "\n",
    "# obtain percentile differences against actual\n",
    "march_perf['diff_with_75th_percentile_cashflow'] = march_perf['75th_percentile_cashflow'] - march_perf['ground_truth']\n",
    "march_perf['diff_with_90th_percentile_cashflow'] = march_perf['90th_percentile_cashflow'] - march_perf['ground_truth']\n",
    "march_perf['diff_with_95th_percentile_cashflow'] = march_perf['95th_percentile_cashflow'] - march_perf['ground_truth']\n",
    "march_perf['diff_with_100th_percentile_cashflow'] = march_perf['100th_percentile_cashflow'] - march_perf['ground_truth']\n",
    "\n",
    "march_perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp, split into tertiles\n",
    "group_values = [100000, 300000, 500000]\n",
    "subgroups_df = pd.DataFrame(columns=march_perf.columns.tolist() + ['cashflow_group', 'tertile'])\n",
    "\n",
    "for idx, val in enumerate(group_values):\n",
    "    if idx == 0:\n",
    "        increment = 20000\n",
    "    elif idx == 1:\n",
    "        increment = 60000\n",
    "    else:\n",
    "        increment = 100000\n",
    "        \n",
    "    lower_bound, upper_bound = val - increment, val + increment\n",
    "    filtered_gt_df = march_perf.loc[(march_perf['ground_truth'] >= lower_bound) & (march_perf['ground_truth'] <= upper_bound), :]\n",
    "    \n",
    "    # assign new columns\n",
    "    filtered_gt_df['cashflow_group'] = val\n",
    "    filtered_gt_df['tertile'] = pd.qcut(filtered_gt_df['diff_with_100th_percentile_cashflow'], 3, labels=['low', 'med', 'high'])\n",
    "    \n",
    "    subgroups_df = subgroups_df.append(filtered_gt_df)\n",
    "    \n",
    "subgroups_df.index.name = 'outlet_id'\n",
    "    \n",
    "print(subgroups_df.shape)\n",
    "subgroups_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups_df[subgroups_df.cashflow_group == 500000].tertile.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get relevant columns for outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_ids = subgroups_df.index.tolist()\n",
    "outlet_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['outlet_id', 'area', 'regional', 'branch', 'kabupaten', 'kecamatan', 'kelurahan','klasifikasi', 'tipe_outlet'] \n",
    "details = spark.read.table('dmp_remote.outlet_dim_dd') \\\n",
    "    .filter((F.col('outlet_id').isin(outlet_ids)) & (F.col('load_date').between('2020-03-01', '2020-03-31'))) \\\n",
    "    .select(cols).distinct().toPandas().set_index('outlet_id')\n",
    "\n",
    "details.index = details.index.astype(int)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with outlet_ids\n",
    "print(subgroups_df.shape)\n",
    "joined = subgroups_df.join(details, how='left')\n",
    "print(joined.shape)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops rows with duplicates\n",
    "joined = joined.loc[~joined.index.duplicated(), :]\n",
    "print(joined.shape)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values_cols = [f'q{num}_threshold' for num in range(1,5)]\n",
    "joined['quartile'] = pd.cut(joined.performance_percentile, bins=[0.0, 25.0, 50.0, 75.0, 100.0], labels=['q1','q2','q3','q4'])\n",
    "joined[q_values_cols] = joined.training_ground_truths_list.apply(lambda lst: pd.Series({f\"q{num}_value\": round(np.quantile(lst, num*0.25),2) for num in range(1,5)}))\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = [\n",
    "    'cashflow_group',\n",
    "    'tertile',\n",
    "    'diff_with_100th_percentile_threshold'\n",
    "]\n",
    "\n",
    "cols = [\n",
    "    'outlet_id',\n",
    "    'ground_truth',\n",
    "    'performance_percentile',\n",
    "    'quartile',\n",
    "    'q1_threshold',\n",
    "    'q2_threshold',\n",
    "    'q3_threshold',\n",
    "    'q4_threshold',\n",
    "    '90th_percentile_cashflow',\n",
    "    '95th_percentile_cashflow',\n",
    "    'diff_with_75th_percentile_cashflow',\n",
    "    'diff_with_90th_percentile_cashflow',\n",
    "    'diff_with_95th_percentile_cashflow',\n",
    "    'diff_with_100th_percentile_cashflow',\n",
    "    'area',\n",
    "    'regional',\n",
    "    'branch',\n",
    "    'kabupaten',\n",
    "    'kecamatan',\n",
    "    'kelurahan',\n",
    "    'klasifikasi',\n",
    "    'tipe_outlet',\n",
    "    'cashflow_group',\n",
    "    'tertile',\n",
    "]\n",
    "\n",
    "selected = joined.reset_index()[cols].rename(columns={\n",
    "    'diff_with_75th_percentile_cashflow': 'diff_with_75th_percentile_threshold',\n",
    "    'diff_with_90th_percentile_cashflow': 'diff_with_90th_percentile_threshold',\n",
    "    'diff_with_95th_percentile_cashflow': 'diff_with_95th_percentile_threshold',\n",
    "    'diff_with_100th_percentile_cashflow': 'diff_with_100th_percentile_threshold',\n",
    "    '90th_percentile_cashflow': '90th_percentile_threshold',\n",
    "    '95th_percentile_cashflow': '95th_percentile_threshold'}).sort_values(\n",
    "        by=sort_by\n",
    "    )\n",
    "\n",
    "# round to 2 dp\n",
    "selected = selected.round(2)\n",
    "\n",
    "# sort values\n",
    "selected = selected.sort_values(['cashflow_group', 'tertile', 'diff_with_100th_percentile_threshold', 'performance_percentile'])\n",
    "\n",
    "print(selected.shape)\n",
    "selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "selected.to_csv('/home/cdsw/outlets_for_pilots_full_set_expanded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "1. Check number of zero cashflow days in May 2020 do not exceed by 5\n",
    "2. Check outlets' month total cashflow for March 2020 and May 2020 do not deviate by more than 15%\n",
    "2. Split the pool of outlets into groups and manually balance them to ensure similar distribution of region and total cashflow\n",
    "    - using High and Low tertile for each cashflow group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
